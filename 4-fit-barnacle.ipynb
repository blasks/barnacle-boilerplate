{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e260a5a1-686a-491d-8b6c-dc7fcb17787a",
   "metadata": {},
   "source": [
    "# Step 4: Fit Barnacle model to your data\n",
    "\n",
    "Use this notebook to fit the Barnacle model to your normalized data tensor. Fitting Barnacle to data requires tuning two model parameters: \n",
    "1. `R` -- the number of components\n",
    "1. `lambda` -- the sparsity parameter\n",
    "\n",
    "There are many methods for fitting model parameters. The cross-validated parameter search strategy here is the method used to fit Barnacle to metatranscriptomic data in the [original Barnacle manuscript](https://doi.org/10.1101/2024.07.15.603627). This strategy aims to reduce resource costs by fitting `R` first and then `lambda`, rather than both parameters simultaneously. It also depends on sample replicates for performing cross validation. If your data does not have sample replicates, you might instead consider trying split-half analysis for parameter selection, but this method is not supported by this notebook.\n",
    "\n",
    "Please refer to the notebook [3-tensorize-data.ipynb](https://github.com/blasks/barnacle-boilerplate/blob/main/3-tensorize-data.ipynb) for proper formatting of your input data tensor. Note that in order to facilitate bootstrapping, sample ID and replicate ID are combined into a unique identifier called `'sample_replicate_id'` (how creative). This will be the name of the third mode of your tensor. The sample ID and replicate ID information is preserved in separate metadata arrays in the dataset. The script will use this information to shuffle replicates between bootstraps, which enables more robust parameter selection, and confidence intervals in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tomli_w\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# suppress unhelpful warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "# set color palette\n",
    "sns.set_palette(sns.color_palette([\n",
    "    '#9B5DE5', '#FFAC69', '#00C9AE', '#FD3F92', '#0F0A0A', '#959AB1', '#FFDB66', '#FFB1CA', '#63B9FF', '#4F1DD7'\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef448ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUTS -- edit these variables as needed\n",
    "\n",
    "# filepath of your input data tensor (e.g. 'directory/example-tensor.nc')\n",
    "datapath = \"/scratch/bgrodner/iron_ko_contigs/metat_search_results/barnacle/iron_KOs.txt-tidy_all_trim/sub_taxa_01/data-tensor.nc\"\n",
    "\n",
    "# output directory where produced files will be saved (e.g. 'data/'\n",
    "outdir = (\n",
    "    \"/scratch/bgrodner/iron_ko_contigs/metat_search_results/barnacle/iron_KOs.txt-tidy_all_trim/sub_taxa_01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00f1e7-fc34-4c74-a31d-52e18de160fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check inputs\n",
    "\n",
    "# read in data tensor\n",
    "ds = xr.load_dataset(datapath)\n",
    "\n",
    "# check input data formats\n",
    "required_vars = ['data', 'sample_id', 'replicate_id', 'sample_replicate_id']\n",
    "for var in required_vars:\n",
    "    if var not in list(ds.variables):\n",
    "        raise Exception(f\"Tensor missing variable '{var}'. See `3-tensorize-data.ipynb` for proper formatting of input tensor dataset.\")\n",
    "if input(f'Found tensor with the following modes and dimensions:\\n{dict(ds.sizes)}\\nIs this correct? (Y/N):').strip().lower() == 'n':\n",
    "    raise Exception(f'Please double check input tensor dataset.')\n",
    "else:\n",
    "    modes = list(ds.data.coords)\n",
    "\n",
    "# check output directory\n",
    "outdir = f\"{outdir}/barnacle\"\n",
    "if not os.path.isdir(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# check sparsity modes\n",
    "sparse_modes = int(input(f'How many modes will sparsity be applied to? (Enter 0/1/2/3, default is 1):'))\n",
    "if sparse_modes == 1:\n",
    "    if input(f'Sparsity will be applied to mode 1-{modes[0]}\\nIs this correct? (Y/N):').strip().lower() == 'n':\n",
    "        raise Exception('Sparsity constraint should be applied to the first mode in the tensor. Please rearrange input tensor.')\n",
    "elif sparse_modes == 2:\n",
    "    if input(f'Sparsity will be applied to modes 1-{modes[0]} and 2-{modes[1]}\\nIs this correct? (Y/N):').strip().lower() == 'n':\n",
    "        raise Exception('Sparsity constraints should be applied to first and second modes in the tensor. Please rearrange input tensor.')\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77233bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample_names = ds.sample_id.to_numpy()\n",
    "np.all(sample_names[:-1] <= sample_names[1:])\n",
    "names, counts = np.unique(sample_names, return_counts=True)\n",
    "counts.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cbe95-ccc1-4704-8252-26ab316e95f7",
   "metadata": {},
   "source": [
    "### Part A: Identifying optimal rank\n",
    "\n",
    "In this step you will identify the optimal rank for your model. The rank dictates the number of components in the model. If you will be using Barnacle for cluster analysis, the rank directly corresponds to the number of clusters output by the model.\n",
    "\n",
    "The optimal value of rank can be identified via cross-validation. You will fit a series of models with different rank values, and then compare how well these models reproduce held out replicates from your dataset. To isolate the rank parameter from the sparsity parameter, this first cross-validated parameter search is done without applying any sparsity to the model.\n",
    "\n",
    "Identifying the optimal rank requires the following steps:\n",
    "1. Enter a list of rank values you would like to assess. A good starting range might be 1 - 30 in intervals of 5.\n",
    "1. Run the `grid-search.py` script using the config file generated by this notebook (`rank-search.toml`).\n",
    "    - You should run this script from the command line, outside of this notebook.\n",
    "    - This step can last anywhere from minutes to days, or more, depending on the size of your data, the rank values you are testing, and the specs of your computer system. If it is taking a long time to run, consider testing a smaller set of rank values, running fewer bootstraps, running on a more powerful computer, or distilling your data down to a subset you are most interested in analyzing.\n",
    "1. Look for a minimum in the cross-validated sum of squared errors (SSE).\n",
    "    - If you do not see a minimum, you may need to repeat these steps, testing a broader range of rank values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be1275-ad9b-4119-8ec0-af48a8504cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUTS -- edit these variables as needed\n",
    "\n",
    "# list values of R (rank) to test\n",
    "ranks = [1,5,10,20,40,80, 160]\n",
    "\n",
    "# list which modes of the tensor model should be constrained to be non-negative (modes are 0-indexed)\n",
    "nonneg_modes = [1, 2] # default is [1, 2] (i.e. first mode unconstrained, other two modes must be nonnegative values)\n",
    "\n",
    "# enter random seed (integer)\n",
    "seed = 9481\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44728140-9d5e-463e-95df-2c8d98f0f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build config file\n",
    "\n",
    "# max number of replicates\n",
    "_, counts = np.unique(ds.sample_id.data, return_counts=True)\n",
    "max_rep = np.max(counts)\n",
    "reps = [str(r) for r in range(max_rep)]\n",
    "# replst = ['A','B','C','D','E','F','G']\n",
    "# reps = replst[:max_rep]\n",
    "\n",
    "# config file structure\n",
    "config = {\n",
    "    \"grid\": {\n",
    "        \"ranks\": ranks,\n",
    "        \"lambdas\": [\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ],\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"nonneg_modes\": nonneg_modes,\n",
    "        \"tol\": 0.00001,\n",
    "        \"n_iter_max\": 10000,\n",
    "        \"n_initializations\": 5,\n",
    "    },\n",
    "    \"script\": {\n",
    "        \"input\": datapath,\n",
    "        \"outdir\": f\"{outdir}/fitting\",\n",
    "        \"n_bootstraps\": 10,\n",
    "        \"replicates\": reps,\n",
    "        \"max_processes\": 80,\n",
    "        \"seed\": seed,\n",
    "    },\n",
    "}\n",
    "\n",
    "# function to save config as toml file\n",
    "def save_toml(config, filename=\"config.toml\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        tomli_w.dump(config, f)\n",
    "    print(f\"TOML file '{filename}' created successfully.\")\n",
    "\n",
    "# save and display config toml file\n",
    "fn_out = f\"{outdir}/rank-search.toml\"\n",
    "save_toml(config, filename=fn_out)\n",
    "print(f\"\\n{tomli_w.dumps(config)}\")\n",
    "\n",
    "# print command to initiate rank search\n",
    "print(\n",
    "    f\"\"\"Run the following command from the command line:\n",
    "\n",
    "    poetry run python grid-search.py {fn_out}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e99e89-1e66-4383-a803-237fc9a24d0b",
   "metadata": {},
   "source": [
    "#### Initialize cross-valiated rank search\n",
    "\n",
    "Run the above command from your computer's command line. This step may take anywhere from minutes to days to complete. Once it has successfully completed, it should have produced a number of directories and files within your output directory, including the following csv file that contains the cross-validated SSE data:\n",
    "\n",
    "`fitting/cv_data.csv`\n",
    "\n",
    "You can then run the following code to look for a minimum in the cross-validated SSE data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d0cb5-d4fa-41e6-b37b-f9e460e93cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the result of the cross validation run\n",
    "\n",
    "# read in cross-validation data\n",
    "rank_df = pd.read_csv(f\"{outdir}/fitting/cv_data.csv\")\n",
    "# down-select to just cross-validation data with lambda=0\n",
    "rank_df = rank_df[(rank_df['modeled_replicate'] != rank_df['comparison_replicate']) & rank_df['lambda'].eq(0.)].reset_index(drop=True)\n",
    "rank_df['Replicate Pair'] = rank_df['modeled_replicate'].astype(str) + ', ' + rank_df['comparison_replicate'].astype(str)\n",
    "rank_df['Bootstrap'] = rank_df.bootstrap_id.astype(str)\n",
    "\n",
    "# calculate minimum average cross-validated SSE\n",
    "avg_cv_sse = rank_df.groupby('rank').sse.mean()\n",
    "rank_min_sse = int(avg_cv_sse.idxmin())\n",
    "print(f'The minimum average cross-validated SSE was {avg_cv_sse.min():.3}, acheived with R={rank_min_sse}.')\n",
    "# if rank_min_sse == rank_df['rank'].min():\n",
    "#     if rank_min_sse == 1:\n",
    "#         print('This may indicate a high level of noise in your data. You may need to reconsider preprocessing and normalization.')\n",
    "#     else:\n",
    "#         print('Please re-run the cross-validated rank search to include lower values of rank.')\n",
    "# elif rank_min_sse == rank_df['rank'].max():\n",
    "#     print('Please re-run the cross-validated rank search to include higher values of rank.')\n",
    "# else:\n",
    "#     print(f'This indicates an optimal rank value of {optimal_rank}. Proceed to select the optimal value of lambda.')\n",
    "\n",
    "# plot cross-validated SSE as a function of rank\n",
    "fig, axis = plt.subplots(figsize=(10,5))\n",
    "sns.scatterplot(data=rank_df, x='rank', y='sse', hue='Bootstrap', style='Replicate Pair', ax=axis, legend=True)\n",
    "sns.lineplot(data=rank_df, x='rank', y='sse', color=sns.color_palette()[4], errorbar='sd', ax=axis)\n",
    "axis.legend(loc='center left', bbox_to_anchor=(1,0.5));\n",
    "axis.set(xlabel='Rank (Number of Components)', ylabel='Cross-Validated SSE'); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f544d-9eb6-42d9-be4e-b44d7813c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_df['mean_cluster_size'] = (1 - rank_df['mode0_factor_sparsity']) * ds.sizes[modes[0]]\n",
    "summary_df = rank_df.groupby('rank')[[ 'sse', 'fms', 'mean_cluster_size', 'bootstrap_id']].agg(\n",
    "    sse=('sse', 'mean'), \n",
    "    sse_sem=('sse', 'sem'), \n",
    "    fms=('fms', 'mean'), \n",
    "    fms_sem=('fms', 'sem'), \n",
    "    mean_cluster_size=('mean_cluster_size', 'mean'),\n",
    "    n_bootstraps=('bootstrap_id', pd.Series.nunique)\n",
    ")\n",
    "display(summary_df.sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bdcbd-217f-406e-965c-9262bf0cc1f5",
   "metadata": {},
   "source": [
    "### Part B: Identifying optimal sparsity\n",
    "\n",
    "In this step you will identify the optimal sparsity for your model. Sparsity can be applied to 0, 1, 2, or all three modes of your model. The default is to apply sparsity just to the first mode, which should correspond to the variable you want to cluster (e.g. genes in the case of metatranscriptomic data). It is important that the modes with sparsity constraints are the first ones listed, so please re-orient your input data tensor if that is not the case.\n",
    "\n",
    "The optimal sparsity parameter (lambda) will be identified with another cross-validated parameter search. You will fit a series of models with different lambda values, and then compare how well these models reproduce held out replicates from your dataset. All models in this search will be fit with rank set to the optimal value identified in Part A.\n",
    "\n",
    "Identifying the optimal lambda requires the following steps:\n",
    "1. Enter a list of lambda values you would like to assess.\n",
    "    - Powers of 2 or 10 are a good idea for an initial coarse search.\n",
    "    - A linear search is a good idea to fine-tune the optimal value of lambda.\n",
    "1. Run the `grid-search.py` script using the config file generated by this notebook (`lambda-search.toml`).\n",
    "    - You should run this script from the command line, outside of this notebook.\n",
    "    - This step can last anywhere from minutes to days, or more, depending on the size of your data, the lambda values you are testing, and the specs of your computer system. However it is often faster than the rank search since sparse models are faster to fit.\n",
    "1. Look for an optimum in the cross-validated data. There is no single best rule for this selection, but two rules of thumb are offered below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bae9d9-3b88-4ed9-97d8-1db2fc1be81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUTS -- edit these variables as needed\n",
    "\n",
    "# list values of lambda (sparsity penalty) to test\n",
    "# lambdas = [0., 0.01, 0.1, 1., 10.] # powers of ten (coarse search)\n",
    "\n",
    "# lambdas = [0., 1., 2., 4., 8., 16., 32.] \n",
    "lambdas = [0.05, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8] # powers of two\n",
    "# lambdas = [0., 0.5, 1., 1.5, 2., 2.5, 3., 3.5, 4., 4.5, 5.] # linear (fine-tuned search)\n",
    "\n",
    "# list wich modes of the tensor model should have sparsity applied to them (modes are 0-indexed)\n",
    "sparsity_modes = [0] # default is [0] (i.e. sparsity applied to first mode only)\n",
    "\n",
    "# enter optimal rank (identified in part A)\n",
    "# optimal_rank = 80\n",
    "optimal_rank = rank_min_sse\n",
    "\n",
    "# enter random seed (integer)\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb177b6-6272-400e-b3db-f014dfb1e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build config file\n",
    "\n",
    "# update ranks and lambdas\n",
    "config['grid']['ranks'] = [optimal_rank]\n",
    "config['grid']['lambdas'] = [[float(l) if i in sparsity_modes else 0.0 for i in [0, 1, 2]] for l in lambdas]\n",
    "\n",
    "# update random seed\n",
    "config['script']['seed'] = seed\n",
    "\n",
    "# save and display config toml file\n",
    "save_toml(config, filename=f\"{outdir}/lambda-search.toml\")\n",
    "print(f\"\\n{tomli_w.dumps(config)}\")\n",
    "\n",
    "# print command to initiate sparsity parameter search\n",
    "print(\n",
    "    f\"\"\"Run the following command from the command line:\n",
    "\n",
    "    poetry run python grid-search.py {outdir}/config-lambda-search.toml\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0ba81-b6e1-4cc3-90c9-e1126cb54205",
   "metadata": {},
   "source": [
    "#### Initialize cross-valiated sparsity parameter search\n",
    "\n",
    "Run the above command from your computer's command line. This step may take anywhere from minutes to days to complete. Once it has successfully completed, it should have produced a number of directories and files within your output directory, and should have updated the `fitting/cv_data.csv` to included cross-validated data from models with the different sparsity parameters tested. These data include:\n",
    "- Sum of Squared Errors (SSE): Measures how well a model fit to one subset of your data reproduces another subset of your data held out during fitting\n",
    "- Factor Match Score (FMS): Measures the similarity components between models fit to different subsets of your data\n",
    "\n",
    "You can then run the following code to help identify the optimal lambda based on the cross-validation data. There is no single best rule for this selection, but here are two rules of thumb you could consider:\n",
    "- Minimum SSE: As when selecting rank, select the lambda that corresponds to the lowest average cross-validated SSE. This heuristic tends to lead to the most conservative choice of lambda.\n",
    "- Maximum FMS: FMS measures the similarity of components between models fit to different subsets of your data. When you're interested in clusters this FMS can be thought to measure the robustness and reproducibility of clusters. This heuristic tends to lead to higher values of lambda than SSE and thus sparser models.\n",
    "\n",
    "Additionally, to account for variation in the bootstraps, you can use the \"one standard error rule\" (see e.g. Hastie, Tibshirani and Friedman, 2009) in combination with either the min SSE or max FMS heuristic. This convention suggests that instead of taking the true optimum (min SSE or max FMS), we take the highest value of lambda that lays within one standard error of the optimum. The second block of code calculates optimal values of lambda based on this heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab39088-aadc-41c0-ad2d-e47af7267e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate SSE and FMS optima and visualize the result of the cross validation run\n",
    "\n",
    "# read in cross-validation data\n",
    "lambda_df = pd.read_csv(f\"{outdir}/fitting/cv_data.csv\")\n",
    "# down-select to just cross-validation data with rank=optimal_rank\n",
    "lambda_df = lambda_df[\n",
    "    (lambda_df['modeled_replicate'] != lambda_df['comparison_replicate']) & \n",
    "    (lambda_df['rank'].eq(optimal_rank))\n",
    "].reset_index(drop=True)\n",
    "lambda_df['Replicate Pair'] = lambda_df['modeled_replicate'] + ', ' + lambda_df['comparison_replicate']\n",
    "lambda_df['Bootstrap'] = lambda_df.bootstrap_id.astype(str)\n",
    "lambda_df['mean_cluster_size'] = (1 - lambda_df['mode0_factor_sparsity']) * ds.sizes[modes[0]]\n",
    "\n",
    "# calculate min SSE and max FMS\n",
    "summary_df = lambda_df.groupby('lambda')[['rank', 'sse', 'fms', 'mean_cluster_size', 'bootstrap_id']].agg(\n",
    "    rank=('rank', 'mean'), \n",
    "    sse=('sse', 'mean'), \n",
    "    sse_sem=('sse', 'sem'), \n",
    "    fms=('fms', 'mean'), \n",
    "    fms_sem=('fms', 'sem'), \n",
    "    mean_cluster_size=('mean_cluster_size', 'mean'),\n",
    "    n_bootstraps=('bootstrap_id', pd.Series.nunique)\n",
    ")\n",
    "summary_df\n",
    "print(f'With rank={rank_min_sse}, the minimum mean SSE was {summary_df.sse.min():.3}, acheived with λ={summary_df.sse.idxmin()}.')\n",
    "print(f'With rank={rank_min_sse}, the maximum mean FMS was {summary_df.fms.max():.3}, acheived with λ={summary_df.fms.idxmax()}.')\n",
    "\n",
    "# find optimal lambda based on one standard error rule\n",
    "print('Using the 1 standard error rule:')\n",
    "# one standard error rule for SSE\n",
    "lamb_sse_1se = summary_df[summary_df.sse.lt(summary_df.sse.min() + summary_df.loc[summary_df.sse.idxmin(), 'sse_sem'])].index.max()\n",
    "print(f\"\\tThe optimal λ based on SSE is {lamb_sse_1se} (SSE={summary_df.loc[lamb_sse_1se, 'sse']:.3})\")\n",
    "# one standard error rule for FMS\n",
    "lamb_fms_1se = summary_df[summary_df.fms.gt(summary_df.fms.max() - summary_df.loc[summary_df.fms.idxmax(), 'fms_sem'])].index.max()\n",
    "print(f\"\\tThe optimal λ based on FMS is {lamb_fms_1se} (FMS={summary_df.loc[lamb_fms_1se, 'fms']:.3})\")\n",
    "\n",
    "# display summary data as a table\n",
    "display(summary_df.reset_index())\n",
    "\n",
    "# plot cross-validated SSE and FMS as a function of lambda\n",
    "fig, axes = plt.subplots(3, 1, sharex=True, figsize=(10, 15))\n",
    "for i, metric in enumerate(['sse', 'fms', 'mean_cluster_size']):\n",
    "    sns.scatterplot(data=lambda_df, x='lambda', y=metric, hue='Bootstrap', style='Replicate Pair', ax=axes[i], legend=(i==2))\n",
    "    sns.lineplot(data=lambda_df, x='lambda', y=metric, color=sns.color_palette()[4], errorbar='sd', ax=axes[i])\n",
    "    if i==2:\n",
    "        axes[i].legend(loc='center left', bbox_to_anchor=(1, 1.7));\n",
    "        axes[i].set(\n",
    "            title='Mean Cluster Size vs. λ', xlabel='λ (Sparsity Coefficient)', xscale='log', \n",
    "            ylabel=f'Mean Cluster Size ({modes[0]}s)');\n",
    "    else:\n",
    "        axes[i].set(title=f\"{metric.upper()} vs. λ\", xlabel='λ (Sparsity Coefficient)', xscale='log', ylabel=metric.upper());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e23897-ce10-4a50-b86c-1d876daafc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a41826-1d3d-48bc-a318-f1704e8ee8a3",
   "metadata": {},
   "source": [
    "### Part C: Generating the final model\n",
    "\n",
    "Once you've selected optimal rank and lambda parameters, then model fitting is done! At this point, it is fine to proceed to analysis and visualization using the optimally-parameterized models generated during the parameter grid search. However, it can be helpful to generate more bootstraps of your final model, in order to produce better estimates of model variation/robustness, and to assign confidence estimates to the composition of clusters, as well as their associations with the patterns detected in each model component. \n",
    "\n",
    "To generate more boostraps of your final model, run the `grid-search.py` script one more time using the config file generated below (`final-model.toml`). Unless you are running a very large number of bootstraps, this should finish running more quickly than in parts A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdaa84-b8cf-4c2a-a8d8-e91ea7388c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUTS -- edit these variables as needed\n",
    "\n",
    "# enter optimal rank (identified in part A)\n",
    "optimal_rank = 100\n",
    "# optimal_rank = rank_min_sse\n",
    "\n",
    "# enter optimal lambda (identified in part B)\n",
    "optimal_lambda = 1.0\n",
    "# optimal_lambda = lamb_fms_1se\n",
    "\n",
    "# enter number of bootstraps to run with optimal parameters\n",
    "n_bootstraps = 100\n",
    "\n",
    "# enter random seed (integer)\n",
    "seed = 525600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936deea0-ed69-40ed-8684-475362cfa3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build config file\n",
    "\n",
    "# update ranks and lambdas\n",
    "config['grid']['ranks'] = [optimal_rank]\n",
    "config['grid']['lambdas'] = [[float(optimal_lambda) if i in sparsity_modes else 0.0 for i in [0, 1, 2]]]\n",
    "\n",
    "# update bootstraps and random seed\n",
    "config['script']['n_bootstraps'] = n_bootstraps\n",
    "config['script']['seed'] = seed\n",
    "\n",
    "# save and display config toml file\n",
    "save_toml(config, filename=f\"{outdir}/final-model.toml\")\n",
    "print(f\"\\n{tomli_w.dumps(config)}\")\n",
    "\n",
    "# print command to initiate sparsity parameter search\n",
    "print(\n",
    "    f\"\"\"Run the following command from the command line:\n",
    "\n",
    "    poetry run python grid-search.py {outdir}/final-model.toml\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031b0b1-9d96-4628-bab2-1ee8a46cf973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
